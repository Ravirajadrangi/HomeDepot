{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance\n",
    "The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabGraph Create\n",
    "This notebook uses the LabGraph create machine learning iPython module. You need a personal licence to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This non-commercial license of GraphLab Create is assigned to thomasv1000@hotmail.fr and will expire on October 12, 2016. For commercial licensing options, visit https://dato.com/buy/.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-34988 - Server binary: /Users/tjaskula/.graphlab/anaconda/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1455060377.log\n",
      "[INFO] GraphLab Server Version: 1.8.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/train.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.128079 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,str,str,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/train.csv\n",
      "PROGRESS: Parsing completed. Parsed 74067 lines in 0.173978 secs.\n"
     ]
    }
   ],
   "source": [
    "train = gl.SFrame.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/test.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.211768 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,int,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/test.csv\n",
      "PROGRESS: Parsing completed. Parsed 166693 lines in 0.337596 secs.\n"
     ]
    }
   ],
   "source": [
    "test = gl.SFrame.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/product_descriptions.csv\n",
      "PROGRESS: Parsing completed. Parsed 100 lines in 0.538864 secs.\n",
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n",
      "PROGRESS: Read 61134 lines. Lines per second: 54594.6\n",
      "PROGRESS: Finished parsing file /Users/tjaskula/Documents/GitHub/Kaggle.HomeDepot/data/product_descriptions.csv\n",
      "PROGRESS: Parsing completed. Parsed 124428 lines in 1.74218 secs.\n"
     ]
    }
   ],
   "source": [
    "desc = gl.SFrame.read_csv(\"../data/product_descriptions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge train with description\n",
    "train = train.join(desc, on = 'product_uid', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge test with description\n",
    "test = test.join(desc, on = 'product_uid', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's explore some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's examine 3 different queries and products:\n",
    "* first from the training set\n",
    "* somewhere in the moddle in the training set\n",
    "* the last one from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'product_description': 'Not only do angles make joints stronger, they also provide more consistent, straight corners. Simpson Strong-Tie offers a wide variety of angles in various sizes and thicknesses to handle light-duty jobs or projects where a structural connection is needed. Some can be bent (skewed) to match the project. For outdoor projects or those where moisture is present, use our ZMAX zinc-coated connectors, which provide extra resistance against corrosion (look for a \"Z\" at the end of the model number).Versatile connector for various 90 connections and home repair projectsStronger than angled nailing or screw fastening aloneHelp ensure joints are consistently straight and strongDimensions: 3 in. x 3 in. x 1-1/2 in.Made from 12-Gauge steelGalvanized for extra corrosion resistanceInstall with 10d common nails or #9 x 1-1/2 in. Strong-Drive SD screws',\n",
       " 'product_title': 'Simpson Strong-Tie 12-Gauge Angle',\n",
       " 'product_uid': 100001,\n",
       " 'relevance': 3.0,\n",
       " 'search_term': 'angle bracket'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = train[0]\n",
    "first_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'angle bracket'** search term is not contained in the body. **'angle'** would be after stemming however **'bracket'** is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 113228,\n",
       " 'product_description': 'PureBond Plywood Project Panels are a convenient and cost-effective way to build cabinets, furniture and other woodworking projects. It provides a beautiful wood veneer face bonded to a strong and flat wood core. These PureBond Project Panels are made with no added formaldehyde, eliminating the concern about off-gassing dangerous fumes during fabrication or when installed in your home. Their smaller size makes them easy to handle and allows you to order just the amount of wood you need. PureBond plywood, in Project Panels sizes or in full sheet sizes, are a Home Depot exclusive.California residents: see&nbsp;Proposition 65 informationDecorative mahogany veneer applied to both sides of this panelB-2 plain sliced mahogany - 7-ply constructionLight weight, all-wood veneer constructionPrecision-cut hardwood plywood panels in convenient small sizesCommon: 3/4 in. x 2 ft. x 4 ft.; Actual: 0.703 in. x 24 in. x 48 in.Grade: B-2',\n",
       " 'product_title': '3/4 in. x 2 ft. x 4 ft. PureBond Mahogany Plywood Project Panel',\n",
       " 'product_uid': 137334,\n",
       " 'relevance': 3.0,\n",
       " 'search_term': 'table top wood'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_doc = train[37033]\n",
    "middle_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only **'wood'** is present from search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 221473,\n",
       " 'product_description': 'No. 918 Millennial Ryan heathered texture semi-sheer curtain is a casual solid that adds freshness and a finishing touch to any decor setting. Enhances privacy while allowing light to gently filter through. Clean, simple one-pocket pole top design can be used with a standard or decorative curtain rod. Mix and match with other solids and prints for a look that is all your own.Sheer panel, gently filters lightNo header pole top panelMachine washableWide array of colors to choose from100% polyesterContains 1-curtain panel',\n",
       " 'product_title': 'LICHTENBERG Pool Blue No. 918 Millennial Ryan Heathered Texture Sheer Curtain Panel, 40 in. W x 63 in. L',\n",
       " 'product_uid': 206650,\n",
       " 'relevance': 2.33,\n",
       " 'search_term': 'fine sheer curtain 63 inches'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_doc = train[-1]\n",
    "last_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'sheer'** and **'courtain'** are present and that's all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many search terms are not present in description and title for ranked 3 documents\n",
    "Ranked 3 documents are the most relevents searches, but how many search queries doesn't include the searched term in the description and the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------------------------+\n",
      "|  id | product_uid |         product_title         |\n",
      "+-----+-------------+-------------------------------+\n",
      "|  2  |    100001   | Simpson Strong-Tie 12-Gaug... |\n",
      "|  9  |    100002   | BEHR Premium Textured Deck... |\n",
      "|  18 |    100006   | Whirlpool 1.9 cu. ft. Over... |\n",
      "|  21 |    100006   | Whirlpool 1.9 cu. ft. Over... |\n",
      "|  27 |    100009   | House of Fara 3/4 in. x 3 ... |\n",
      "|  35 |    100011   | Toro Personal Pace Recycle... |\n",
      "|  37 |    100011   | Toro Personal Pace Recycle... |\n",
      "|  65 |    100016   | Sunjoy Calais 8 ft. x 5 ft... |\n",
      "| 123 |    100023   | Quikrete 80 lb. Crack-Resi... |\n",
      "| 162 |    100029   | DecoArt Americana Decor 16... |\n",
      "+-----+-------------+-------------------------------+\n",
      "+--------------------------------+-----------+-------------------------------+\n",
      "|          search_term           | relevance |      product_description      |\n",
      "+--------------------------------+-----------+-------------------------------+\n",
      "|         angle bracket          |    3.0    | Not only do angles make jo... |\n",
      "|           deck over            |    3.0    | BEHR Premium Textured DECK... |\n",
      "|         convection otr         |    3.0    | Achieving delicious result... |\n",
      "|           microwaves           |    3.0    | Achieving delicious result... |\n",
      "|            mdf 3/4             |    3.0    | Get the House of Fara 3/4 ... |\n",
      "| briggs and stratton lawn mower |    3.0    | Recycler 22 in. Personal P... |\n",
      "|            gas mowe            |    3.0    | Recycler 22 in. Personal P... |\n",
      "|          grill gazebo          |    3.0    | Make grilling great with t... |\n",
      "| CONCRETE & MASONRY CLEANER...  |    3.0    | Quikrete 80 lb. Crack-Resi... |\n",
      "|          chalk paint           |    3.0    | Achieving a vintage, time-... |\n",
      "+--------------------------------+-----------+-------------------------------+\n",
      "+-------------------------------+\n",
      "|     search_term_word_count    |\n",
      "+-------------------------------+\n",
      "|   {'bracket': 1, 'angle': 1}  |\n",
      "|     {'over': 1, 'deck': 1}    |\n",
      "|  {'otr': 1, 'convection': 1}  |\n",
      "|       {'microwaves': 1}       |\n",
      "|      {'mdf': 1, '3/4': 1}     |\n",
      "| {'and': 1, 'stratton': 1, ... |\n",
      "|     {'gas': 1, 'mowe': 1}     |\n",
      "|   {'grill': 1, 'gazebo': 1}   |\n",
      "| {'etcher': 1, 'cleaner': 1... |\n",
      "|    {'chalk': 1, 'paint': 1}   |\n",
      "+-------------------------------+\n",
      "[10 rows x 7 columns]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['search_term_word_count'] = gl.text_analytics.count_words(train['search_term'])\n",
    "ranked3doc = train[train['relevance'] == 3]\n",
    "print ranked3doc.head()\n",
    "len(ranked3doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_search = gl.text_analytics.tokenize(ranked3doc['search_term'], to_lower = True)\n",
    "words_description = gl.text_analytics.tokenize(ranked3doc['product_description'], to_lower = True)\n",
    "words_title = gl.text_analytics.tokenize(ranked3doc['product_title'], to_lower = True)\n",
    "wordsdiff_desc = []\n",
    "wordsdiff_title = []\n",
    "puid = []\n",
    "search_term = []\n",
    "ws_count = []\n",
    "ws_count_used_desc = []\n",
    "ws_count_used_title = []\n",
    "for item in xrange(len(ranked3doc)):\n",
    "    ws = words_search[item]\n",
    "    pd = words_description[item]\n",
    "    pt = words_title[item]\n",
    "    diff = set(ws) - set(pd)\n",
    "    if diff is None:\n",
    "        diff = 0\n",
    "    wordsdiff_desc.append(diff)\n",
    "    \n",
    "    diff2 = set(ws) - set(pt)\n",
    "    if diff2 is None:\n",
    "        diff2 = 0\n",
    "    wordsdiff_title.append(diff2)\n",
    "    \n",
    "    puid.append(ranked3doc[item]['product_uid'])\n",
    "    search_term.append(ranked3doc[item]['search_term'])\n",
    "    ws_count.append(len(ws))\n",
    "    ws_count_used_desc.append(len(ws) - len(diff))\n",
    "    ws_count_used_title.append(len(ws) - len(diff2))\n",
    "    \n",
    "differences = gl.SFrame({\"puid\" : puid,\n",
    "                         \"search term\": search_term,\n",
    "                         \"diff desc\" : wordsdiff_desc,\n",
    "                         \"diff title\" : wordsdiff_title,\n",
    "                         \"ws count\" : ws_count, \n",
    "                         \"ws count used desc\" : ws_count_used_desc,\n",
    "                         \"ws count used title\" : ws_count_used_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">diff desc</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">diff title</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">puid</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">search term</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ws count</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ws count used desc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[recycling, bins]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[recycling, bins]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">145727</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">recycling bins</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[over, deck]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[over, deck]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">100002</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">deck over</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[hammer, electric, drill]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[hammer, electric, drill]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">120061</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">electric hammer drill</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[microwaves]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[microwaves]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">100006</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">microwaves</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[plywoods]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[plywoods]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">119996</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">plywoods</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[coca, cola]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[coca, cola]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">120276</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">coca cola</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[greenhouses]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[greenhouses]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">120318</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">greenhouses</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[pipe, cutters]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[pipe, cutters]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">119840</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">pipe cutters</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[buit, themostat, in]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[buit, themostat, in]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">206359</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">buit in themostat</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[mowers, ridding]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[mowers, ridding]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">120366</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ridding mowers</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ws count used title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[19125 rows x 7 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tdiff desc\tlist\n",
       "\tdiff title\tlist\n",
       "\tpuid\tint\n",
       "\tsearch term\tstr\n",
       "\tws count\tint\n",
       "\tws count used desc\tint\n",
       "\tws count used title\tint\n",
       "\n",
       "Rows: 19125\n",
       "\n",
       "Data:\n",
       "+---------------------------+---------------------------+--------+\n",
       "|         diff desc         |         diff title        |  puid  |\n",
       "+---------------------------+---------------------------+--------+\n",
       "|     [recycling, bins]     |     [recycling, bins]     | 145727 |\n",
       "|        [over, deck]       |        [over, deck]       | 100002 |\n",
       "| [hammer, electric, drill] | [hammer, electric, drill] | 120061 |\n",
       "|        [microwaves]       |        [microwaves]       | 100006 |\n",
       "|         [plywoods]        |         [plywoods]        | 119996 |\n",
       "|        [coca, cola]       |        [coca, cola]       | 120276 |\n",
       "|       [greenhouses]       |       [greenhouses]       | 120318 |\n",
       "|      [pipe, cutters]      |      [pipe, cutters]      | 119840 |\n",
       "|   [buit, themostat, in]   |   [buit, themostat, in]   | 206359 |\n",
       "|     [mowers, ridding]     |     [mowers, ridding]     | 120366 |\n",
       "+---------------------------+---------------------------+--------+\n",
       "+-----------------------+----------+--------------------+---------------------+\n",
       "|      search term      | ws count | ws count used desc | ws count used title |\n",
       "+-----------------------+----------+--------------------+---------------------+\n",
       "|     recycling bins    |    2     |         0          |          0          |\n",
       "|       deck over       |    2     |         0          |          0          |\n",
       "| electric hammer drill |    3     |         0          |          0          |\n",
       "|       microwaves      |    1     |         0          |          0          |\n",
       "|        plywoods       |    1     |         0          |          0          |\n",
       "|       coca cola       |    2     |         0          |          0          |\n",
       "|      greenhouses      |    1     |         0          |          0          |\n",
       "|      pipe cutters     |    2     |         0          |          0          |\n",
       "|   buit in themostat   |    3     |         0          |          0          |\n",
       "|     ridding mowers    |    2     |         0          |          0          |\n",
       "+-----------------------+----------+--------------------+---------------------+\n",
       "[19125 rows x 7 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences.sort(['ws count used desc', 'ws count used title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No terms used in description : 2666\n",
      "No terms used in title : 2152\n",
      "No terms used in description and title : 1206\n"
     ]
    }
   ],
   "source": [
    "print \"No terms used in description : \" + str(len(differences[differences['ws count used desc'] == 0]))\n",
    "print \"No terms used in title : \" + str(len(differences[differences['ws count used title'] == 0]))\n",
    "print \"No terms used in description and title : \" + str(len(differences[(differences['ws count used desc'] == 0) & \n",
    "                                                                        (differences['ws count used title'] == 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stemmer = SnowballStemmer(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "def stem(word):\n",
    "    singles = [stemmer.stem(plural) for plural in unicode(word, errors='replace').split()]\n",
    "    text = ' '.join(singles)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stemming train search term...\n",
      "Starting stemming train product description...\n",
      "Starting stemming train product title...\n",
      "Starting stemming test search term...\n",
      "Starting stemming test product description...\n",
      "Starting stemming test product title...\n"
     ]
    }
   ],
   "source": [
    "print \"Starting stemming train search term...\"\n",
    "stemmed = train['search_term'].apply(stem)\n",
    "train['stem_search_term'] = stemmed\n",
    "\n",
    "print \"Starting stemming train product description...\"\n",
    "stemmed = train['product_description'].apply(stem)\n",
    "train['stem_product_description'] = stemmed\n",
    "\n",
    "print \"Starting stemming train product title...\"\n",
    "stemmed = train['product_title'].apply(stem)\n",
    "train['stem_product_title'] = stemmed\n",
    "\n",
    "print \"Starting stemming test search term...\"\n",
    "stemmed = test['search_term'].apply(stem)\n",
    "test['stem_search_term'] = stemmed\n",
    "\n",
    "print \"Starting stemming test product description...\"\n",
    "stemmed = test['product_description'].apply(stem)\n",
    "test['stem_product_description'] = stemmed\n",
    "\n",
    "print \"Starting stemming test product title...\"\n",
    "stemmed = test['product_title'].apply(stem)\n",
    "test['stem_product_title'] = stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['stem_search_term_split'] = train['stem_search_term'].apply(lambda x: x.split())\n",
    "train['stem_product_title_split'] = train['stem_product_title'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_bm25_title = gl.text_analytics.bm25(train['stem_product_title_split'], train['stem_search_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">doc_id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">bm25</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0066090612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">32</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.5763494627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">33</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.5763494627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">34</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.5763494627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">35</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.47279286788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">36</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.47279286788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">368</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8.41944329073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">423</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6.37098731693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">672</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.43939408383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">673</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.43939408383</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[? rows x 2 columns]<br/>Note: Only the head of the SFrame is printed. This SFrame is lazily evaluated.<br/>You can use len(sf) to force materialization.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tdoc_id\tint\n",
       "\tbm25\tfloat\n",
       "\n",
       "Rows: Unknown\n",
       "\n",
       "Data:\n",
       "+--------+---------------+\n",
       "| doc_id |      bm25     |\n",
       "+--------+---------------+\n",
       "|   2    |  5.0066090612 |\n",
       "|   32   |  6.5763494627 |\n",
       "|   33   |  6.5763494627 |\n",
       "|   34   |  6.5763494627 |\n",
       "|   35   | 4.47279286788 |\n",
       "|   36   | 4.47279286788 |\n",
       "|  368   | 8.41944329073 |\n",
       "|  423   | 6.37098731693 |\n",
       "|  672   | 5.43939408383 |\n",
       "|  673   | 5.43939408383 |\n",
       "+--------+---------------+\n",
       "[? rows x 2 columns]\n",
       "Note: Only the head of the SFrame is printed. This SFrame is lazily evaluated.\n",
       "You can use len(sf) to force materialization."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bm25_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['product_desc_word_count'] = gl.text_analytics.count_words(train['stem_product_description'])\n",
    "train_desc_tfidf = gl.text_analytics.tf_idf(train['product_desc_word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['desc_tfidf'] = train_desc_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['product_title_word_count'] = gl.text_analytics.count_words(train['stem_product_title'])\n",
    "train_title_tfidf = gl.text_analytics.tf_idf(train['product_title_word_count'])\n",
    "train['title_tfidf'] = train_title_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['distance_desc'] = train.apply(lambda x: gl.distances.cosine(x['search_tfidf'],x['desc_tfidf']))\n",
    "#train['distance_desc_sqrt'] = train['distance_desc'] ** 2\n",
    "train['distance_title'] = train.apply(lambda x: gl.distances.cosine(x['search_tfidf'],x['title_tfidf']))\n",
    "#train['distance_title_sqrt'] = train['distance_title'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Random forest regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 74067\n",
      "PROGRESS: Number of features          : 2\n",
      "PROGRESS: Number of unpacked features : 2\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | Iteration | Elapsed Time | Training-max_error | Training-rmse |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n",
      "PROGRESS: | 1         | 0.102436     | 2.49883            | 1.95229       |\n",
      "PROGRESS: | 2         | 0.115938     | 2.4951             | 1.94864       |\n",
      "PROGRESS: | 3         | 0.129281     | 2.49137            | 1.94499       |\n",
      "PROGRESS: | 4         | 0.141645     | 2.48839            | 1.94134       |\n",
      "PROGRESS: | 5         | 0.154205     | 2.48519            | 1.93769       |\n",
      "PROGRESS: | 6         | 0.166078     | 2.48199            | 1.93405       |\n",
      "PROGRESS: | 7         | 0.178179     | 2.4788             | 1.9304        |\n",
      "PROGRESS: | 8         | 0.191280     | 2.47516            | 1.92676       |\n",
      "PROGRESS: | 9         | 0.204245     | 2.4718             | 1.92313       |\n",
      "PROGRESS: | 10        | 0.216407     | 2.46859            | 1.91948       |\n",
      "PROGRESS: | 11        | 0.228840     | 2.46526            | 1.91584       |\n",
      "PROGRESS: | 12        | 0.242061     | 2.46239            | 1.9122        |\n",
      "PROGRESS: | 13        | 0.255052     | 2.45951            | 1.90855       |\n",
      "PROGRESS: | 14        | 0.266656     | 2.45619            | 1.90492       |\n",
      "PROGRESS: | 15        | 0.279718     | 2.45328            | 1.90127       |\n",
      "PROGRESS: | 16        | 0.292480     | 2.45003            | 1.89764       |\n",
      "PROGRESS: | 17        | 0.304539     | 2.44683            | 1.894         |\n",
      "PROGRESS: | 18        | 0.316264     | 2.4435             | 1.89037       |\n",
      "PROGRESS: | 19        | 0.328166     | 2.4403             | 1.88673       |\n",
      "PROGRESS: | 20        | 0.341071     | 2.4371             | 1.88309       |\n",
      "PROGRESS: | 21        | 0.354071     | 2.43417            | 1.87944       |\n",
      "PROGRESS: | 22        | 0.367248     | 2.43085            | 1.87581       |\n",
      "PROGRESS: | 23        | 0.379041     | 2.42752            | 1.87218       |\n",
      "PROGRESS: | 24        | 0.392087     | 2.42459            | 1.86854       |\n",
      "PROGRESS: | 25        | 0.405611     | 2.42127            | 1.86492       |\n",
      "PROGRESS: | 26        | 0.416898     | 2.41793            | 1.86129       |\n",
      "PROGRESS: | 27        | 0.430127     | 2.41502            | 1.85765       |\n",
      "PROGRESS: | 28        | 0.442160     | 2.4118             | 1.85401       |\n",
      "PROGRESS: | 29        | 0.455182     | 2.40885            | 1.85038       |\n",
      "PROGRESS: | 30        | 0.468286     | 2.40595            | 1.84675       |\n",
      "PROGRESS: | 31        | 0.480960     | 2.40263            | 1.84312       |\n",
      "PROGRESS: | 32        | 0.492592     | 2.39938            | 1.83949       |\n",
      "PROGRESS: | 33        | 0.504033     | 2.39606            | 1.83587       |\n",
      "PROGRESS: | 34        | 0.516595     | 2.39272            | 1.83225       |\n",
      "PROGRESS: | 35        | 0.530701     | 2.38983            | 1.82862       |\n",
      "PROGRESS: | 36        | 0.542227     | 2.3865             | 1.825         |\n",
      "PROGRESS: | 37        | 0.555130     | 2.3836             | 1.82137       |\n",
      "PROGRESS: | 38        | 0.568025     | 2.38056            | 1.81774       |\n",
      "PROGRESS: | 39        | 0.580688     | 2.37731            | 1.81412       |\n",
      "PROGRESS: | 40        | 0.594139     | 2.37442            | 1.81049       |\n",
      "PROGRESS: | 41        | 0.606299     | 2.3711             | 1.80687       |\n",
      "PROGRESS: | 42        | 0.619202     | 2.36822            | 1.80325       |\n",
      "PROGRESS: | 43        | 0.632050     | 2.3649             | 1.79963       |\n",
      "PROGRESS: | 44        | 0.645027     | 2.36157            | 1.79602       |\n",
      "PROGRESS: | 45        | 0.658286     | 2.35824            | 1.7924        |\n",
      "PROGRESS: | 46        | 0.672454     | 2.35491            | 1.78879       |\n",
      "PROGRESS: | 47        | 0.687157     | 2.35206            | 1.78516       |\n",
      "PROGRESS: | 48        | 0.700666     | 2.34884            | 1.78155       |\n",
      "PROGRESS: | 49        | 0.715117     | 2.34558            | 1.77794       |\n",
      "PROGRESS: | 50        | 0.728505     | 2.34225            | 1.77432       |\n",
      "PROGRESS: | 51        | 0.742768     | 2.33905            | 1.7707        |\n",
      "PROGRESS: | 52        | 0.756206     | 2.33572            | 1.76709       |\n",
      "PROGRESS: | 53        | 0.768632     | 2.33252            | 1.76347       |\n",
      "PROGRESS: | 54        | 0.781558     | 2.32962            | 1.75985       |\n",
      "PROGRESS: | 55        | 0.793316     | 2.3263             | 1.75624       |\n",
      "PROGRESS: | 56        | 0.806405     | 2.32337            | 1.75262       |\n",
      "PROGRESS: | 57        | 0.818924     | 2.32004            | 1.74901       |\n",
      "PROGRESS: | 58        | 0.831903     | 2.31672            | 1.7454        |\n",
      "PROGRESS: | 59        | 0.846853     | 2.31381            | 1.74179       |\n",
      "PROGRESS: | 60        | 0.859179     | 2.31048            | 1.73818       |\n",
      "PROGRESS: | 61        | 0.870566     | 2.30715            | 1.73458       |\n",
      "PROGRESS: | 62        | 0.883104     | 2.30382            | 1.73097       |\n",
      "PROGRESS: | 63        | 0.896406     | 2.30057            | 1.72736       |\n",
      "PROGRESS: | 64        | 0.909974     | 2.29763            | 1.72375       |\n",
      "PROGRESS: | 65        | 0.922126     | 2.29431            | 1.72015       |\n",
      "PROGRESS: | 66        | 0.936142     | 2.29106            | 1.71655       |\n",
      "PROGRESS: | 67        | 0.949449     | 2.28785            | 1.71294       |\n",
      "PROGRESS: | 68        | 0.963015     | 2.28492            | 1.70932       |\n",
      "PROGRESS: | 69        | 0.976192     | 2.28171            | 1.70571       |\n",
      "PROGRESS: | 70        | 0.990656     | 2.27856            | 1.70212       |\n",
      "PROGRESS: | 71        | 1.006162     | 2.27537            | 1.69851       |\n",
      "PROGRESS: | 72        | 1.022291     | 2.27247            | 1.6949        |\n",
      "PROGRESS: | 73        | 1.036628     | 2.26926            | 1.69129       |\n",
      "PROGRESS: | 74        | 1.051161     | 2.26633            | 1.68769       |\n",
      "PROGRESS: | 75        | 1.065139     | 2.26342            | 1.68408       |\n",
      "PROGRESS: | 76        | 1.077151     | 2.26021            | 1.68048       |\n",
      "PROGRESS: | 77        | 1.089177     | 2.25689            | 1.67688       |\n",
      "PROGRESS: | 78        | 1.102249     | 2.25367            | 1.67328       |\n",
      "PROGRESS: | 79        | 1.115868     | 2.25078            | 1.66967       |\n",
      "PROGRESS: | 80        | 1.128527     | 2.24753            | 1.66608       |\n",
      "PROGRESS: | 81        | 1.139857     | 2.2442             | 1.66248       |\n",
      "PROGRESS: | 82        | 1.151589     | 2.2414             | 1.65888       |\n",
      "PROGRESS: | 83        | 1.162871     | 2.23808            | 1.65529       |\n",
      "PROGRESS: | 84        | 1.175656     | 2.23476            | 1.65171       |\n",
      "PROGRESS: | 85        | 1.188476     | 2.23151            | 1.64812       |\n",
      "PROGRESS: | 86        | 1.201181     | 2.22819            | 1.64453       |\n",
      "PROGRESS: | 87        | 1.213552     | 2.22495            | 1.64095       |\n",
      "PROGRESS: | 88        | 1.225304     | 2.22163            | 1.63736       |\n",
      "PROGRESS: | 89        | 1.236792     | 2.2183             | 1.63378       |\n",
      "PROGRESS: | 90        | 1.249765     | 2.21544            | 1.63018       |\n",
      "PROGRESS: | 91        | 1.262303     | 2.21211            | 1.6266        |\n",
      "PROGRESS: | 92        | 1.274239     | 2.20892            | 1.62301       |\n",
      "PROGRESS: | 93        | 1.286786     | 2.20571            | 1.61942       |\n",
      "PROGRESS: | 94        | 1.299859     | 2.20238            | 1.61584       |\n",
      "PROGRESS: | 95        | 1.312242     | 2.19918            | 1.61224       |\n",
      "PROGRESS: | 96        | 1.324306     | 2.19596            | 1.60866       |\n",
      "PROGRESS: | 97        | 1.337810     | 2.19276            | 1.60508       |\n",
      "PROGRESS: | 98        | 1.350785     | 2.1895             | 1.6015        |\n",
      "PROGRESS: | 99        | 1.362768     | 2.1863             | 1.59791       |\n",
      "PROGRESS: | 100       | 1.374829     | 2.18309            | 1.59432       |\n",
      "PROGRESS: | 101       | 1.388702     | 2.18016            | 1.59074       |\n",
      "PROGRESS: | 102       | 1.402437     | 2.17696            | 1.58715       |\n",
      "PROGRESS: | 103       | 1.413939     | 2.17364            | 1.58359       |\n",
      "PROGRESS: | 104       | 1.427116     | 2.17071            | 1.58          |\n",
      "PROGRESS: | 105       | 1.438899     | 2.16746            | 1.57643       |\n",
      "PROGRESS: | 106       | 1.451936     | 2.16451            | 1.57285       |\n",
      "PROGRESS: | 107       | 1.464699     | 2.16157            | 1.56927       |\n",
      "PROGRESS: | 108       | 1.477318     | 2.15875            | 1.56569       |\n",
      "PROGRESS: | 109       | 1.489437     | 2.15555            | 1.56211       |\n",
      "PROGRESS: | 110       | 1.501853     | 2.1523             | 1.55855       |\n",
      "PROGRESS: | 111       | 1.513344     | 2.14897            | 1.55499       |\n",
      "PROGRESS: | 112       | 1.526367     | 2.14572            | 1.55142       |\n",
      "PROGRESS: | 113       | 1.539369     | 2.14278            | 1.54785       |\n",
      "PROGRESS: | 114       | 1.550852     | 2.13945            | 1.54429       |\n",
      "PROGRESS: | 115       | 1.563878     | 2.13655            | 1.54072       |\n",
      "PROGRESS: | 116       | 1.575445     | 2.13323            | 1.53716       |\n",
      "PROGRESS: | 117       | 1.586983     | 2.13001            | 1.5336        |\n",
      "PROGRESS: | 118       | 1.600289     | 2.12709            | 1.53004       |\n",
      "PROGRESS: | 119       | 1.612152     | 2.12376            | 1.52648       |\n",
      "PROGRESS: | 120       | 1.625246     | 2.1209             | 1.52292       |\n",
      "PROGRESS: | 121       | 1.638223     | 2.11802            | 1.51935       |\n",
      "PROGRESS: | 122       | 1.650827     | 2.1147             | 1.51579       |\n",
      "PROGRESS: | 123       | 1.664135     | 2.11157            | 1.51224       |\n",
      "PROGRESS: | 124       | 1.675584     | 2.10824            | 1.50869       |\n",
      "PROGRESS: | 125       | 1.689456     | 2.10533            | 1.50513       |\n",
      "PROGRESS: | 126       | 1.701335     | 2.10213            | 1.50156       |\n",
      "PROGRESS: | 127       | 1.712791     | 2.0988             | 1.49802       |\n",
      "PROGRESS: | 128       | 1.724277     | 2.09548            | 1.49447       |\n",
      "PROGRESS: | 129       | 1.735849     | 2.09215            | 1.49092       |\n",
      "PROGRESS: | 130       | 1.747697     | 2.08895            | 1.48737       |\n",
      "PROGRESS: | 131       | 1.759350     | 2.08562            | 1.48382       |\n",
      "PROGRESS: | 132       | 1.772169     | 2.08242            | 1.48027       |\n",
      "PROGRESS: | 133       | 1.783883     | 2.07909            | 1.47673       |\n",
      "PROGRESS: | 134       | 1.795762     | 2.07628            | 1.47318       |\n",
      "PROGRESS: | 135       | 1.808557     | 2.07303            | 1.46964       |\n",
      "PROGRESS: | 136       | 1.821690     | 2.07011            | 1.46609       |\n",
      "PROGRESS: | 137       | 1.833829     | 2.06721            | 1.46254       |\n",
      "PROGRESS: | 138       | 1.847396     | 2.06389            | 1.459         |\n",
      "PROGRESS: | 139       | 1.859345     | 2.06068            | 1.45545       |\n",
      "PROGRESS: | 140       | 1.872061     | 2.05736            | 1.45192       |\n",
      "PROGRESS: | 141       | 1.883381     | 2.05403            | 1.44839       |\n",
      "PROGRESS: | 142       | 1.894956     | 2.05071            | 1.44486       |\n",
      "PROGRESS: | 143       | 1.907207     | 2.04751            | 1.44132       |\n",
      "PROGRESS: | 144       | 1.920509     | 2.04418            | 1.43779       |\n",
      "PROGRESS: | 145       | 1.933419     | 2.04124            | 1.43425       |\n",
      "PROGRESS: | 146       | 1.946501     | 2.03831            | 1.43071       |\n",
      "PROGRESS: | 147       | 1.958078     | 2.03499            | 1.42719       |\n",
      "PROGRESS: | 148       | 1.970866     | 2.03174            | 1.42367       |\n",
      "PROGRESS: | 149       | 1.982941     | 2.02884            | 1.42013       |\n",
      "PROGRESS: | 150       | 1.995579     | 2.02551            | 1.41661       |\n",
      "PROGRESS: | 151       | 2.007507     | 2.02218            | 1.41309       |\n",
      "PROGRESS: | 152       | 2.020659     | 2.01929            | 1.40956       |\n",
      "PROGRESS: | 153       | 2.032354     | 2.01596            | 1.40604       |\n",
      "PROGRESS: | 154       | 2.044406     | 2.01263            | 1.40252       |\n",
      "PROGRESS: | 155       | 2.057670     | 2.00971            | 1.399         |\n",
      "PROGRESS: | 156       | 2.071681     | 2.00651            | 1.39547       |\n",
      "PROGRESS: | 157       | 2.084813     | 2.00362            | 1.39195       |\n",
      "PROGRESS: | 158       | 2.098037     | 2.00073            | 1.38843       |\n",
      "PROGRESS: | 159       | 2.109969     | 1.99751            | 1.38491       |\n",
      "PROGRESS: | 160       | 2.121436     | 1.99419            | 1.3814        |\n",
      "PROGRESS: | 161       | 2.134310     | 1.99104            | 1.37789       |\n",
      "PROGRESS: | 162       | 2.147803     | 1.98783            | 1.37438       |\n",
      "PROGRESS: | 163       | 2.160023     | 1.98463            | 1.37086       |\n",
      "PROGRESS: | 164       | 2.172029     | 1.98131            | 1.36735       |\n",
      "PROGRESS: | 165       | 2.183853     | 1.9781             | 1.36383       |\n",
      "PROGRESS: | 166       | 2.196483     | 1.97477            | 1.36033       |\n",
      "PROGRESS: | 167       | 2.209104     | 1.97145            | 1.35683       |\n",
      "PROGRESS: | 168       | 2.221063     | 1.96812            | 1.35334       |\n",
      "PROGRESS: | 169       | 2.233745     | 1.96478            | 1.34984       |\n",
      "PROGRESS: | 170       | 2.246161     | 1.96153            | 1.34635       |\n",
      "PROGRESS: | 171       | 2.257713     | 1.95819            | 1.34285       |\n",
      "PROGRESS: | 172       | 2.270457     | 1.95538            | 1.33935       |\n",
      "PROGRESS: | 173       | 2.282524     | 1.95252            | 1.33585       |\n",
      "PROGRESS: | 174       | 2.295232     | 1.94933            | 1.33235       |\n",
      "PROGRESS: | 175       | 2.307768     | 1.94609            | 1.32887       |\n",
      "PROGRESS: | 176       | 2.321046     | 1.94287            | 1.32537       |\n",
      "PROGRESS: | 177       | 2.332459     | 1.93954            | 1.32189       |\n",
      "PROGRESS: | 178       | 2.344652     | 1.93673            | 1.31839       |\n",
      "PROGRESS: | 179       | 2.356097     | 1.9334             | 1.31491       |\n",
      "PROGRESS: | 180       | 2.369141     | 1.93052            | 1.31142       |\n",
      "PROGRESS: | 181       | 2.380742     | 1.92719            | 1.30794       |\n",
      "PROGRESS: | 182       | 2.392640     | 1.92399            | 1.30446       |\n",
      "PROGRESS: | 183       | 2.405969     | 1.92066            | 1.30099       |\n",
      "PROGRESS: | 184       | 2.418382     | 1.91735            | 1.29752       |\n",
      "PROGRESS: | 185       | 2.430846     | 1.91402            | 1.29405       |\n",
      "PROGRESS: | 186       | 2.444004     | 1.91113            | 1.29056       |\n",
      "PROGRESS: | 187       | 2.455552     | 1.9078             | 1.2871        |\n",
      "PROGRESS: | 188       | 2.467297     | 1.90448            | 1.28364       |\n",
      "PROGRESS: | 189       | 2.478823     | 1.90114            | 1.28017       |\n",
      "PROGRESS: | 190       | 2.490808     | 1.89793            | 1.2767        |\n",
      "PROGRESS: | 191       | 2.503623     | 1.89504            | 1.27323       |\n",
      "PROGRESS: | 192       | 2.516526     | 1.89213            | 1.26975       |\n",
      "PROGRESS: | 193       | 2.527957     | 1.88881            | 1.2663        |\n",
      "PROGRESS: | 194       | 2.540269     | 1.88548            | 1.26284       |\n",
      "PROGRESS: | 195       | 2.552355     | 1.88228            | 1.25938       |\n",
      "PROGRESS: | 196       | 2.564128     | 1.87896            | 1.25593       |\n",
      "PROGRESS: | 197       | 2.577096     | 1.87608            | 1.25247       |\n",
      "PROGRESS: | 198       | 2.589720     | 1.87283            | 1.24902       |\n",
      "PROGRESS: | 199       | 2.601700     | 1.86951            | 1.24558       |\n",
      "PROGRESS: | 200       | 2.614022     | 1.8663             | 1.24212       |\n",
      "PROGRESS: | 201       | 2.627016     | 1.86342            | 1.23867       |\n",
      "PROGRESS: | 202       | 2.639099     | 1.86022            | 1.23521       |\n",
      "PROGRESS: | 203       | 2.652184     | 1.85701            | 1.23176       |\n",
      "PROGRESS: | 204       | 2.663756     | 1.85367            | 1.22832       |\n",
      "PROGRESS: | 205       | 2.676694     | 1.85046            | 1.22487       |\n",
      "PROGRESS: | 206       | 2.688300     | 1.84713            | 1.22144       |\n",
      "PROGRESS: | 207       | 2.700939     | 1.8438             | 1.21801       |\n",
      "PROGRESS: | 208       | 2.713800     | 1.8409             | 1.21457       |\n",
      "PROGRESS: | 209       | 2.726710     | 1.838              | 1.21114       |\n",
      "PROGRESS: | 210       | 2.739912     | 1.83502            | 1.2077        |\n",
      "PROGRESS: | 211       | 2.751828     | 1.83209            | 1.20426       |\n",
      "PROGRESS: | 212       | 2.764963     | 1.82888            | 1.20083       |\n",
      "PROGRESS: | 213       | 2.776265     | 1.82555            | 1.19741       |\n",
      "PROGRESS: | 214       | 2.789075     | 1.82263            | 1.19398       |\n",
      "PROGRESS: | 215       | 2.801819     | 1.81947            | 1.19057       |\n",
      "PROGRESS: | 216       | 2.815038     | 1.81626            | 1.18714       |\n",
      "PROGRESS: | 217       | 2.826533     | 1.81294            | 1.18373       |\n",
      "PROGRESS: | 218       | 2.837983     | 1.80962            | 1.18033       |\n",
      "PROGRESS: | 219       | 2.850621     | 1.8067             | 1.17691       |\n",
      "PROGRESS: | 220       | 2.862267     | 1.80347            | 1.1735        |\n",
      "PROGRESS: | 221       | 2.874289     | 1.80025            | 1.17009       |\n",
      "PROGRESS: | 222       | 2.886050     | 1.79705            | 1.16668       |\n",
      "PROGRESS: | 223       | 2.899219     | 1.79413            | 1.16327       |\n",
      "PROGRESS: | 224       | 2.911300     | 1.79136            | 1.15986       |\n",
      "PROGRESS: | 225       | 2.923657     | 1.78803            | 1.15647       |\n",
      "PROGRESS: | 226       | 2.937878     | 1.78484            | 1.15307       |\n",
      "PROGRESS: | 227       | 2.950271     | 1.78164            | 1.14967       |\n",
      "PROGRESS: | 228       | 2.963492     | 1.77885            | 1.14627       |\n",
      "PROGRESS: | 229       | 2.976699     | 1.77594            | 1.14288       |\n",
      "PROGRESS: | 230       | 2.990809     | 1.77273            | 1.13949       |\n",
      "PROGRESS: | 231       | 3.003675     | 1.76953            | 1.1361        |\n",
      "PROGRESS: | 232       | 3.016722     | 1.76633            | 1.13271       |\n",
      "PROGRESS: | 233       | 3.029549     | 1.763              | 1.12934       |\n",
      "PROGRESS: | 234       | 3.041148     | 1.75967            | 1.12597       |\n",
      "PROGRESS: | 235       | 3.056331     | 1.75678            | 1.12259       |\n",
      "PROGRESS: | 236       | 3.071120     | 1.75345            | 1.11922       |\n",
      "PROGRESS: | 237       | 3.085038     | 1.75024            | 1.11584       |\n",
      "PROGRESS: | 238       | 3.100982     | 1.747              | 1.11248       |\n",
      "PROGRESS: | 239       | 3.115371     | 1.7438             | 1.10911       |\n",
      "PROGRESS: | 240       | 3.131623     | 1.74059            | 1.10575       |\n",
      "PROGRESS: | 241       | 3.147488     | 1.73727            | 1.1024        |\n",
      "PROGRESS: | 242       | 3.162514     | 1.73395            | 1.09905       |\n",
      "PROGRESS: | 243       | 3.176567     | 1.73063            | 1.09571       |\n",
      "PROGRESS: | 244       | 3.189440     | 1.72772            | 1.09235       |\n",
      "PROGRESS: | 245       | 3.201784     | 1.72448            | 1.08901       |\n",
      "PROGRESS: | 246       | 3.212858     | 1.72128            | 1.08566       |\n",
      "PROGRESS: | 247       | 3.224729     | 1.71807            | 1.08231       |\n",
      "PROGRESS: | 248       | 3.238074     | 1.71492            | 1.07898       |\n",
      "PROGRESS: | 249       | 3.250910     | 1.71172            | 1.07564       |\n",
      "PROGRESS: | 250       | 3.263824     | 1.70839            | 1.07231       |\n",
      "PROGRESS: | 251       | 3.275889     | 1.70558            | 1.06897       |\n",
      "PROGRESS: | 252       | 3.289759     | 1.70238            | 1.06564       |\n",
      "PROGRESS: | 253       | 3.301490     | 1.69916            | 1.06231       |\n",
      "PROGRESS: | 254       | 3.314365     | 1.69584            | 1.059         |\n",
      "PROGRESS: | 255       | 3.326641     | 1.69252            | 1.05569       |\n",
      "PROGRESS: | 256       | 3.338584     | 1.68931            | 1.05237       |\n",
      "PROGRESS: | 257       | 3.351000     | 1.68598            | 1.04906       |\n",
      "PROGRESS: | 258       | 3.364373     | 1.68308            | 1.04574       |\n",
      "PROGRESS: | 259       | 3.376249     | 1.67987            | 1.04243       |\n",
      "PROGRESS: | 260       | 3.388134     | 1.67666            | 1.03912       |\n",
      "PROGRESS: | 261       | 3.399538     | 1.67334            | 1.03583       |\n",
      "PROGRESS: | 262       | 3.412054     | 1.67001            | 1.03254       |\n",
      "PROGRESS: | 263       | 3.424355     | 1.66669            | 1.02925       |\n",
      "PROGRESS: | 264       | 3.436466     | 1.66389            | 1.02596       |\n",
      "PROGRESS: | 265       | 3.448805     | 1.66056            | 1.02268       |\n",
      "PROGRESS: | 266       | 3.461921     | 1.65735            | 1.01939       |\n",
      "PROGRESS: | 267       | 3.473872     | 1.65411            | 1.01612       |\n",
      "PROGRESS: | 268       | 3.486230     | 1.65091            | 1.01284       |\n",
      "PROGRESS: | 269       | 3.499424     | 1.648              | 1.00956       |\n",
      "PROGRESS: | 270       | 3.511685     | 1.64468            | 1.0063        |\n",
      "PROGRESS: | 271       | 3.524972     | 1.64148            | 1.00303       |\n",
      "PROGRESS: | 272       | 3.538261     | 1.63816            | 0.999776      |\n",
      "PROGRESS: | 273       | 3.552250     | 1.63492            | 0.996527      |\n",
      "PROGRESS: | 274       | 3.564385     | 1.6316             | 0.993277      |\n",
      "PROGRESS: | 275       | 3.576501     | 1.62827            | 0.990038      |\n",
      "PROGRESS: | 276       | 3.589283     | 1.62494            | 0.986798      |\n",
      "PROGRESS: | 277       | 3.602402     | 1.62162            | 0.983564      |\n",
      "PROGRESS: | 278       | 3.616812     | 1.61876            | 0.980318      |\n",
      "PROGRESS: | 279       | 3.629766     | 1.61544            | 0.977091      |\n",
      "PROGRESS: | 280       | 3.643170     | 1.61252            | 0.973853      |\n",
      "PROGRESS: | 281       | 3.657542     | 1.60927            | 0.970631      |\n",
      "PROGRESS: | 282       | 3.670155     | 1.60606            | 0.9674        |\n",
      "PROGRESS: | 283       | 3.685479     | 1.60283            | 0.964186      |\n",
      "PROGRESS: | 284       | 3.698948     | 1.5995             | 0.960979      |\n",
      "PROGRESS: | 285       | 3.713974     | 1.59665            | 0.95776       |\n",
      "PROGRESS: | 286       | 3.729359     | 1.59345            | 0.954544      |\n",
      "PROGRESS: | 287       | 3.742192     | 1.59012            | 0.951349      |\n",
      "PROGRESS: | 288       | 3.754587     | 1.58689            | 0.948159      |\n",
      "PROGRESS: | 289       | 3.768345     | 1.58393            | 0.944956      |\n",
      "PROGRESS: | 290       | 3.780290     | 1.58071            | 0.941755      |\n",
      "PROGRESS: | 291       | 3.792424     | 1.57739            | 0.938579      |\n",
      "PROGRESS: | 292       | 3.805554     | 1.57451            | 0.935387      |\n",
      "PROGRESS: | 293       | 3.817429     | 1.57117            | 0.932216      |\n",
      "PROGRESS: | 294       | 3.829926     | 1.56785            | 0.929052      |\n",
      "PROGRESS: | 295       | 3.842384     | 1.56495            | 0.925875      |\n",
      "PROGRESS: | 296       | 3.854283     | 1.56174            | 0.922702      |\n",
      "PROGRESS: | 297       | 3.867706     | 1.55841            | 0.919552      |\n",
      "PROGRESS: | 298       | 3.879067     | 1.55509            | 0.916405      |\n",
      "PROGRESS: | 299       | 3.891399     | 1.55189            | 0.913249      |\n",
      "PROGRESS: | 300       | 3.903995     | 1.54857            | 0.910111      |\n",
      "PROGRESS: | 301       | 3.916736     | 1.54536            | 0.906963      |\n",
      "PROGRESS: | 302       | 3.928192     | 1.54203            | 0.903834      |\n",
      "PROGRESS: | 303       | 3.940684     | 1.53872            | 0.900713      |\n",
      "PROGRESS: | 304       | 3.952109     | 1.53539            | 0.897595      |\n",
      "PROGRESS: | 305       | 3.964468     | 1.5322             | 0.894469      |\n",
      "PROGRESS: | 306       | 3.976682     | 1.52899            | 0.891343      |\n",
      "PROGRESS: | 307       | 3.990046     | 1.52606            | 0.888222      |\n",
      "PROGRESS: | 308       | 4.004627     | 1.52273            | 0.885125      |\n",
      "PROGRESS: | 309       | 4.018651     | 1.51952            | 0.882018      |\n",
      "PROGRESS: | 310       | 4.032833     | 1.51636            | 0.878931      |\n",
      "PROGRESS: | 311       | 4.045213     | 1.51304            | 0.87585       |\n",
      "PROGRESS: | 312       | 4.058431     | 1.51016            | 0.872759      |\n",
      "PROGRESS: | 313       | 4.070989     | 1.50734            | 0.869672      |\n",
      "PROGRESS: | 314       | 4.083953     | 1.50401            | 0.866605      |\n",
      "PROGRESS: | 315       | 4.097991     | 1.50113            | 0.863529      |\n",
      "PROGRESS: | 316       | 4.111259     | 1.49788            | 0.860479      |\n",
      "PROGRESS: | 317       | 4.123964     | 1.49489            | 0.857412      |\n",
      "PROGRESS: | 318       | 4.137182     | 1.49206            | 0.854354      |\n",
      "PROGRESS: | 319       | 4.150388     | 1.48921            | 0.851301      |\n",
      "PROGRESS: | 320       | 4.163303     | 1.48629            | 0.848253      |\n",
      "PROGRESS: | 321       | 4.175216     | 1.48309            | 0.845211      |\n",
      "PROGRESS: | 322       | 4.187904     | 1.47988            | 0.842177      |\n",
      "PROGRESS: | 323       | 4.200043     | 1.47668            | 0.839147      |\n",
      "PROGRESS: | 324       | 4.213231     | 1.47347            | 0.836125      |\n",
      "PROGRESS: | 325       | 4.225327     | 1.47027            | 0.833105      |\n",
      "PROGRESS: | 326       | 4.237422     | 1.46703            | 0.83011       |\n",
      "PROGRESS: | 327       | 4.249185     | 1.46369            | 0.827122      |\n",
      "PROGRESS: | 328       | 4.260772     | 1.46036            | 0.82414       |\n",
      "PROGRESS: | 329       | 4.274338     | 1.45759            | 0.821149      |\n",
      "PROGRESS: | 330       | 4.286279     | 1.45427            | 0.818179      |\n",
      "PROGRESS: | 331       | 4.299580     | 1.45106            | 0.8152        |\n",
      "PROGRESS: | 332       | 4.312415     | 1.44774            | 0.812243      |\n",
      "PROGRESS: | 333       | 4.324312     | 1.44442            | 0.809295      |\n",
      "PROGRESS: | 334       | 4.336147     | 1.4411             | 0.806352      |\n",
      "PROGRESS: | 335       | 4.349493     | 1.43789            | 0.8034        |\n",
      "PROGRESS: | 336       | 4.362764     | 1.43502            | 0.800457      |\n",
      "PROGRESS: | 337       | 4.376578     | 1.43178            | 0.797535      |\n",
      "PROGRESS: | 338       | 4.389524     | 1.42845            | 0.794622      |\n",
      "PROGRESS: | 339       | 4.402716     | 1.42557            | 0.791699      |\n",
      "PROGRESS: | 340       | 4.415046     | 1.42224            | 0.788801      |\n",
      "PROGRESS: | 341       | 4.427165     | 1.41894            | 0.78591       |\n",
      "PROGRESS: | 342       | 4.439141     | 1.41607            | 0.78301       |\n",
      "PROGRESS: | 343       | 4.452253     | 1.41315            | 0.780113      |\n",
      "PROGRESS: | 344       | 4.467023     | 1.41028            | 0.777226      |\n",
      "PROGRESS: | 345       | 4.480004     | 1.40695            | 0.774364      |\n",
      "PROGRESS: | 346       | 4.493056     | 1.40362            | 0.771508      |\n",
      "PROGRESS: | 347       | 4.504643     | 1.40028            | 0.768659      |\n",
      "PROGRESS: | 348       | 4.516479     | 1.39695            | 0.76582       |\n",
      "PROGRESS: | 349       | 4.530672     | 1.39387            | 0.76299       |\n",
      "PROGRESS: | 350       | 4.543883     | 1.39054            | 0.760167      |\n",
      "PROGRESS: | 351       | 4.557543     | 1.38771            | 0.757334      |\n",
      "PROGRESS: | 352       | 4.569065     | 1.38438            | 0.754527      |\n",
      "PROGRESS: | 353       | 4.582370     | 1.38153            | 0.751713      |\n",
      "PROGRESS: | 354       | 4.594924     | 1.3782             | 0.748925      |\n",
      "PROGRESS: | 355       | 4.607154     | 1.37487            | 0.746145      |\n",
      "PROGRESS: | 356       | 4.620838     | 1.37195            | 0.743358      |\n",
      "PROGRESS: | 357       | 4.634604     | 1.36863            | 0.740595      |\n",
      "PROGRESS: | 358       | 4.647153     | 1.3653             | 0.737843      |\n",
      "PROGRESS: | 359       | 4.660106     | 1.3621             | 0.735079      |\n",
      "PROGRESS: | 360       | 4.673960     | 1.35926            | 0.732325      |\n",
      "PROGRESS: | 361       | 4.687898     | 1.35594            | 0.7296        |\n",
      "PROGRESS: | 362       | 4.702102     | 1.35306            | 0.726867      |\n",
      "PROGRESS: | 363       | 4.713727     | 1.34974            | 0.724162      |\n",
      "PROGRESS: | 364       | 4.726750     | 1.34683            | 0.721446      |\n",
      "PROGRESS: | 365       | 4.740309     | 1.34402            | 0.718741      |\n",
      "PROGRESS: | 366       | 4.753574     | 1.34098            | 0.716045      |\n",
      "PROGRESS: | 367       | 4.765864     | 1.33777            | 0.713359      |\n",
      "PROGRESS: | 368       | 4.777440     | 1.33445            | 0.7107        |\n",
      "PROGRESS: | 369       | 4.790953     | 1.33111            | 0.70805       |\n",
      "PROGRESS: | 370       | 4.803563     | 1.32778            | 0.705411      |\n",
      "PROGRESS: | 371       | 4.815722     | 1.32454            | 0.702783      |\n",
      "PROGRESS: | 372       | 4.828825     | 1.32167            | 0.700146      |\n",
      "PROGRESS: | 373       | 4.842630     | 1.31846            | 0.69752       |\n",
      "PROGRESS: | 374       | 4.854359     | 1.31514            | 0.694924      |\n",
      "PROGRESS: | 375       | 4.868572     | 1.31194            | 0.69232       |\n",
      "PROGRESS: | 376       | 4.881249     | 1.30869            | 0.689744      |\n",
      "PROGRESS: | 377       | 4.894159     | 1.30585            | 0.687161      |\n",
      "PROGRESS: | 378       | 4.906977     | 1.30252            | 0.684608      |\n",
      "PROGRESS: | 379       | 4.920064     | 1.29963            | 0.682048      |\n",
      "PROGRESS: | 380       | 4.932928     | 1.29632            | 0.679517      |\n",
      "PROGRESS: | 381       | 4.944688     | 1.293              | 0.676998      |\n",
      "PROGRESS: | 382       | 4.957800     | 1.29002            | 0.67447       |\n",
      "PROGRESS: | 383       | 4.969192     | 1.2867             | 0.671976      |\n",
      "PROGRESS: | 384       | 4.979981     | 1.2835             | 0.669474      |\n",
      "PROGRESS: | 385       | 4.992111     | 1.28017            | 0.667002      |\n",
      "PROGRESS: | 386       | 5.003425     | 1.27686            | 0.664543      |\n",
      "PROGRESS: | 387       | 5.015737     | 1.27365            | 0.662079      |\n",
      "PROGRESS: | 388       | 5.028659     | 1.27073            | 0.659625      |\n",
      "PROGRESS: | 389       | 5.041954     | 1.26753            | 0.657185      |\n",
      "PROGRESS: | 390       | 5.054637     | 1.26421            | 0.654777      |\n",
      "PROGRESS: | 391       | 5.066684     | 1.26088            | 0.652381      |\n",
      "PROGRESS: | 392       | 5.079661     | 1.25768            | 0.649979      |\n",
      "PROGRESS: | 393       | 5.092749     | 1.2548             | 0.64759       |\n",
      "PROGRESS: | 394       | 5.105915     | 1.25188            | 0.645214      |\n",
      "PROGRESS: | 395       | 5.117768     | 1.24855            | 0.64287       |\n",
      "PROGRESS: | 396       | 5.129100     | 1.24522            | 0.640541      |\n",
      "PROGRESS: | 397       | 5.142064     | 1.24221            | 0.638203      |\n",
      "PROGRESS: | 398       | 5.154603     | 1.23887            | 0.635898      |\n",
      "PROGRESS: | 399       | 5.167584     | 1.23683            | 0.633592      |\n",
      "PROGRESS: | 400       | 5.180835     | 1.24123            | 0.631299      |\n",
      "PROGRESS: | 401       | 5.192514     | 1.24547            | 0.62904       |\n",
      "PROGRESS: | 402       | 5.206137     | 1.24987            | 0.626776      |\n",
      "PROGRESS: | 403       | 5.218045     | 1.25409            | 0.624545      |\n",
      "PROGRESS: | 404       | 5.229409     | 1.25832            | 0.622331      |\n",
      "PROGRESS: | 405       | 5.240694     | 1.26256            | 0.620132      |\n",
      "PROGRESS: | 406       | 5.253451     | 1.26695            | 0.617924      |\n",
      "PROGRESS: | 407       | 5.265925     | 1.2712             | 0.615753      |\n",
      "PROGRESS: | 408       | 5.278590     | 1.27545            | 0.613597      |\n",
      "PROGRESS: | 409       | 5.292376     | 1.27984            | 0.611439      |\n",
      "PROGRESS: | 410       | 5.304158     | 1.28423            | 0.609315      |\n",
      "PROGRESS: | 411       | 5.316172     | 1.28873            | 0.607188      |\n",
      "PROGRESS: | 412       | 5.329684     | 1.29321            | 0.605076      |\n",
      "PROGRESS: | 413       | 5.341624     | 1.29755            | 0.603001      |\n",
      "PROGRESS: | 414       | 5.353707     | 1.30195            | 0.60092       |\n",
      "PROGRESS: | 415       | 5.366159     | 1.30634            | 0.598857      |\n",
      "PROGRESS: | 416       | 5.377716     | 1.31058            | 0.596834      |\n",
      "PROGRESS: | 417       | 5.390465     | 1.31498            | 0.594806      |\n",
      "PROGRESS: | 418       | 5.403410     | 1.31922            | 0.592813      |\n",
      "PROGRESS: | 419       | 5.417525     | 1.32353            | 0.59084       |\n",
      "PROGRESS: | 420       | 5.429677     | 1.32792            | 0.588864      |\n",
      "PROGRESS: | 421       | 5.441765     | 1.33214            | 0.586929      |\n",
      "PROGRESS: | 422       | 5.454677     | 1.33653            | 0.584988      |\n",
      "PROGRESS: | 423       | 5.467274     | 1.34078            | 0.583085      |\n",
      "PROGRESS: | 424       | 5.479069     | 1.34501            | 0.581202      |\n",
      "PROGRESS: | 425       | 5.491023     | 1.34944            | 0.579336      |\n",
      "PROGRESS: | 426       | 5.503303     | 1.35392            | 0.577469      |\n",
      "PROGRESS: | 427       | 5.516148     | 1.3583             | 0.57562       |\n",
      "PROGRESS: | 428       | 5.528065     | 1.36268            | 0.57379       |\n",
      "PROGRESS: | 429       | 5.541067     | 1.36716            | 0.571979      |\n",
      "PROGRESS: | 430       | 5.552777     | 1.3714             | 0.570208      |\n",
      "PROGRESS: | 431       | 5.565121     | 1.37567            | 0.568456      |\n",
      "PROGRESS: | 432       | 5.577227     | 1.38007            | 0.566703      |\n",
      "PROGRESS: | 433       | 5.588620     | 1.38436            | 0.564991      |\n",
      "PROGRESS: | 434       | 5.600353     | 1.38874            | 0.563278      |\n",
      "PROGRESS: | 435       | 5.612610     | 1.39312            | 0.561587      |\n",
      "PROGRESS: | 436       | 5.624038     | 1.39742            | 0.559936      |\n",
      "PROGRESS: | 437       | 5.636166     | 1.40182            | 0.558284      |\n",
      "PROGRESS: | 438       | 5.649041     | 1.40616            | 0.556674      |\n",
      "PROGRESS: | 439       | 5.661846     | 1.41056            | 0.555085      |\n",
      "PROGRESS: | 440       | 5.673274     | 1.41485            | 0.553518      |\n",
      "PROGRESS: | 441       | 5.686180     | 1.41932            | 0.55195       |\n",
      "PROGRESS: | 442       | 5.699559     | 1.42381            | 0.550403      |\n",
      "PROGRESS: | 443       | 5.712402     | 1.42821            | 0.548898      |\n",
      "PROGRESS: | 444       | 5.725021     | 1.43247            | 0.547418      |\n",
      "PROGRESS: | 445       | 5.738591     | 1.43687            | 0.545958      |\n",
      "PROGRESS: | 446       | 5.752254     | 1.44134            | 0.544499      |\n",
      "PROGRESS: | 447       | 5.763888     | 1.44558            | 0.543083      |\n",
      "PROGRESS: | 448       | 5.775650     | 1.44987            | 0.54169       |\n",
      "PROGRESS: | 449       | 5.788129     | 1.45426            | 0.540298      |\n",
      "PROGRESS: | 450       | 5.800023     | 1.45848            | 0.538951      |\n",
      "PROGRESS: | 451       | 5.811950     | 1.46279            | 0.537627      |\n",
      "PROGRESS: | 452       | 5.824401     | 1.46726            | 0.536303      |\n",
      "PROGRESS: | 453       | 5.836353     | 1.47165            | 0.535004      |\n",
      "PROGRESS: | 454       | 5.850425     | 1.47595            | 0.53375       |\n",
      "PROGRESS: | 455       | 5.864054     | 1.48033            | 0.532498      |\n",
      "PROGRESS: | 456       | 5.877633     | 1.48471            | 0.531292      |\n",
      "PROGRESS: | 457       | 5.890146     | 1.48895            | 0.53011       |\n",
      "PROGRESS: | 458       | 5.903136     | 1.49335            | 0.528951      |\n",
      "PROGRESS: | 459       | 5.916204     | 1.49773            | 0.527796      |\n",
      "PROGRESS: | 460       | 5.929138     | 1.50212            | 0.526666      |\n",
      "PROGRESS: | 461       | 5.942352     | 1.50652            | 0.525582      |\n",
      "PROGRESS: | 462       | 5.955277     | 1.51082            | 0.524523      |\n",
      "PROGRESS: | 463       | 5.968351     | 1.51522            | 0.523467      |\n",
      "PROGRESS: | 464       | 5.982069     | 1.5197             | 0.522436      |\n",
      "PROGRESS: | 465       | 5.994036     | 1.52399            | 0.521453      |\n",
      "PROGRESS: | 466       | 6.006892     | 1.52837            | 0.520472      |\n",
      "PROGRESS: | 467       | 6.019280     | 1.53275            | 0.519519      |\n",
      "PROGRESS: | 468       | 6.031442     | 1.53714            | 0.518612      |\n",
      "PROGRESS: | 469       | 6.044273     | 1.54152            | 0.517731      |\n",
      "PROGRESS: | 470       | 6.057834     | 1.54591            | 0.516875      |\n",
      "PROGRESS: | 471       | 6.071195     | 1.55029            | 0.516025      |\n",
      "PROGRESS: | 472       | 6.082770     | 1.55454            | 0.515222      |\n",
      "PROGRESS: | 473       | 6.095626     | 1.55902            | 0.514423      |\n",
      "PROGRESS: | 474       | 6.108565     | 1.56349            | 0.513651      |\n",
      "PROGRESS: | 475       | 6.121694     | 1.56797            | 0.512906      |\n",
      "PROGRESS: | 476       | 6.133544     | 1.57237            | 0.512187      |\n",
      "PROGRESS: | 477       | 6.146296     | 1.57677            | 0.511518      |\n",
      "PROGRESS: | 478       | 6.158865     | 1.58102            | 0.510875      |\n",
      "PROGRESS: | 479       | 6.171800     | 1.5854             | 0.510237      |\n",
      "PROGRESS: | 480       | 6.185775     | 1.58989            | 0.509626      |\n",
      "PROGRESS: | 481       | 6.197693     | 1.59437            | 0.509043      |\n",
      "PROGRESS: | 482       | 6.210848     | 1.59875            | 0.508488      |\n",
      "PROGRESS: | 483       | 6.222913     | 1.60312            | 0.507981      |\n",
      "PROGRESS: | 484       | 6.235454     | 1.60737            | 0.507502      |\n",
      "PROGRESS: | 485       | 6.247011     | 1.61168            | 0.50705       |\n",
      "PROGRESS: | 486       | 6.259035     | 1.61598            | 0.506626      |\n",
      "PROGRESS: | 487       | 6.272205     | 1.62036            | 0.506208      |\n",
      "PROGRESS: | 488       | 6.285295     | 1.6246             | 0.50584       |\n",
      "PROGRESS: | 489       | 6.298659     | 1.62889            | 0.5055        |\n",
      "PROGRESS: | 490       | 6.311954     | 1.63314            | 0.505187      |\n",
      "PROGRESS: | 491       | 6.325457     | 1.63753            | 0.504881      |\n",
      "PROGRESS: | 492       | 6.338433     | 1.64193            | 0.504625      |\n",
      "PROGRESS: | 493       | 6.351339     | 1.64618            | 0.504396      |\n",
      "PROGRESS: | 494       | 6.363682     | 1.65041            | 0.504196      |\n",
      "PROGRESS: | 495       | 6.377466     | 1.65488            | 0.504002      |\n",
      "PROGRESS: | 496       | 6.390212     | 1.65936            | 0.503836      |\n",
      "PROGRESS: | 497       | 6.401644     | 1.66372            | 0.50372       |\n",
      "PROGRESS: | 498       | 6.415449     | 1.6681             | 0.503611      |\n",
      "PROGRESS: | 499       | 6.428775     | 1.67257            | 0.50353       |\n",
      "PROGRESS: | 500       | 6.440163     | 1.67697            | 0.503478      |\n",
      "PROGRESS: +-----------+--------------+--------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "model1 = gl.random_forest_regression.create(train, target = 'relevance', \n",
    "                                             features = ['distance_desc', 'distance_title'],\n",
    "                                             num_trees = 500,\n",
    "                                             validation_set = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['search_term_word_count'] = gl.text_analytics.count_words(test['stem_search_term'])\n",
    "test_search_tfidf = gl.text_analytics.tf_idf(test['search_term_word_count'])\n",
    "test['search_tfidf'] = test_search_tfidf\n",
    "test['product_desc_word_count'] = gl.text_analytics.count_words(test['stem_product_description'])\n",
    "test_desc_tfidf = gl.text_analytics.tf_idf(test['product_desc_word_count'])\n",
    "test['desc_tfidf'] = test_desc_tfidf\n",
    "test['product_title_word_count'] = gl.text_analytics.count_words(test['stem_product_title'])\n",
    "test_title_tfidf = gl.text_analytics.tf_idf(test['product_title_word_count'])\n",
    "test['title_tfidf'] = test_title_tfidf\n",
    "\n",
    "test['distance_desc'] = test.apply(lambda x: gl.distances.cosine(x['search_tfidf'],x['desc_tfidf']))\n",
    "#test['distance_desc_sqrt'] = test['distance_desc'] ** 2\n",
    "test['distance_title'] = test.apply(lambda x: gl.distances.cosine(x['search_tfidf'],x['title_tfidf']))\n",
    "#test['distance_title_sqrt'] = test['distance_title'] ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npredictions_test = model1.predict(test)\\ntest_errors = predictions_test - test['relevance']\\nRSS_test = sum(test_errors * test_errors)\\nprint RSS_test\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "predictions_test = model1.predict(test)\n",
    "test_errors = predictions_test - test['relevance']\n",
    "RSS_test = sum(test_errors * test_errors)\n",
    "print RSS_test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: float\n",
       "Rows: 166693\n",
       "[2.1928854980468753, 2.1342208251953125, 2.328364990234375, 2.3295657958984375, 2.296669555664063, 2.204806884765625, 2.3182451171875, 2.3572662353515623, 2.18566552734375, 2.603859375, 2.4829770507812503, 2.3647874755859375, 2.4918834228515623, 2.4925120849609375, 2.3430264892578125, 2.3444764404296876, 2.1342208251953125, 2.5022855224609377, 2.2213077392578127, 2.277583740234375, 2.643470458984375, 2.586959228515625, 2.24497705078125, 2.4032442626953125, 2.3497370605468753, 2.46168310546875, 2.1342208251953125, 2.397673095703125, 2.4267039794921876, 2.3365816650390627, 2.5064366455078124, 2.3783542480468753, 2.3080893554687503, 2.391839599609375, 2.4032442626953125, 2.3781336669921878, 2.4060943603515623, 2.4032442626953125, 2.659384521484375, 2.65576025390625, 2.3493861083984378, 2.3401319580078126, 2.27736328125, 2.1342208251953125, 2.4373940429687497, 2.3732879638671873, 2.4854658203125, 2.572867431640625, 2.569054931640625, 2.328517822265625, 2.1299729003906247, 2.1256834716796877, 2.2015145263671876, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1299729003906247, 2.1342208251953125, 2.1342208251953125, 2.2044549560546876, 2.2044549560546876, 2.1342208251953125, 2.385854736328125, 2.3417269287109375, 2.4032442626953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.1342208251953125, 2.3276989746093752, 2.1342208251953125, 2.528997802734375, 2.3104829101562503, 2.1342208251953125, 2.1342208251953125, 2.5186025390625, 2.1342208251953125, 2.385854736328125, 2.33603271484375, 2.33603271484375, 2.297000854492188, 2.27736328125, 2.34004638671875, 2.3700960693359376, 2.4966113281249998, 2.41986474609375, 2.5457342529296874, 2.4032442626953125, 2.1342208251953125, 2.5605966796875, 2.3727390136718753, 2.2294552001953125, 2.2293757324218753, 2.4550633544921876, 2.3284459228515626, 2.2998106689453124, ... ]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_test = model1.predict(test)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result = model1.evaluate(test)\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = gl.SFrame(test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.19288549805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.1342208252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.32836499023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.3295657959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.29666955566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.20480688477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.31824511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.35726623535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">12</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.18566552734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">13</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.603859375</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[166693 rows x 2 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tid\tint\n",
       "\trelevance\tfloat\n",
       "\n",
       "Rows: 166693\n",
       "\n",
       "Data:\n",
       "+----+---------------+\n",
       "| id |   relevance   |\n",
       "+----+---------------+\n",
       "| 1  | 2.19288549805 |\n",
       "| 4  |  2.1342208252 |\n",
       "| 5  | 2.32836499023 |\n",
       "| 6  |  2.3295657959 |\n",
       "| 7  | 2.29666955566 |\n",
       "| 8  | 2.20480688477 |\n",
       "| 10 | 2.31824511719 |\n",
       "| 11 | 2.35726623535 |\n",
       "| 12 | 2.18566552734 |\n",
       "| 13 |  2.603859375  |\n",
       "+----+---------------+\n",
       "[166693 rows x 2 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.add_column(predictions_test)\n",
    "submission.rename({'X1': 'id', 'X2':'relevance'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission['relevance'] = submission.apply(lambda x: 3.0 if x['relevance'] > 3.0 else x['relevance'])\n",
    "submission['relevance'] = submission.apply(lambda x: 1.0 if x['relevance'] < 1.0 else x['relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission['relevance'] = submission.apply(lambda x: str(x['relevance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.export_csv('../data/submission2.csv', quote_level = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gl.canvas.set_target('ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
